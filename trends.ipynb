{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZMysdL+dJJik77SV5uKVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecfairle/trends/blob/main/trends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzmnLbkpGLvq"
      },
      "outputs": [],
      "source": [
        "# Reddit AI Keyword Trend Analysis\n",
        "# This notebook analyzes keyword trends from AI-focused subreddits over the past 6 months\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    print(\"NLTK download failed - continuing without it\")\n",
        "\n",
        "# Reddit API Configuration\n",
        "# Create a Reddit app at https://www.reddit.com/prefs/apps\n",
        "REDDIT_CLIENT_ID = \"your_client_id_here\"\n",
        "REDDIT_CLIENT_SECRET = \"your_client_secret_here\"\n",
        "REDDIT_USER_AGENT = \"AITrendAnalysis/1.0 by YourUsername\"  # Replace with your username\n",
        "\n",
        "class RedditAIAnalyzer:\n",
        "    def __init__(self, client_id, client_secret, user_agent):\n",
        "        \"\"\"Initialize Reddit API client\"\"\"\n",
        "        self.reddit = praw.Reddit(\n",
        "            client_id=client_id,\n",
        "            client_secret=client_secret,\n",
        "            user_agent=user_agent\n",
        "        )\n",
        "\n",
        "        # AI-focused subreddits to analyze\n",
        "        self.ai_subreddits = [\n",
        "            'MachineLearning',      # 2.8M members - academic/research focused\n",
        "            'artificial',           # 200K members - general AI discussion\n",
        "            'singularity',          # 400K members - AI future/AGI discussion\n",
        "            'ChatGPT',             # 300K members - ChatGPT specific\n",
        "            'OpenAI',              # 100K members - OpenAI developments\n",
        "            'LocalLLaMA',          # 150K members - local AI models\n",
        "            'ArtificialIntelligence', # 180K members - general AI\n",
        "            'deeplearning',        # 120K members - deep learning focus\n",
        "            'compsci',             # 500K members - computer science (AI subset)\n",
        "            'MachineLearningNews'  # 50K members - AI news and updates\n",
        "        ]\n",
        "\n",
        "        # Extended stopwords for cleaner analysis\n",
        "        try:\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        except:\n",
        "            self.stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "\n",
        "        self.stop_words.update([\n",
        "            'reddit', 'post', 'comment', 'thread', 'subreddit', 'upvote', 'downvote',\n",
        "            'https', 'http', 'www', 'com', 'co', 'amp', 'via', 'edit', 'update',\n",
        "            'today', 'new', 'great', 'good', 'best', 'check', 'get', 'make',\n",
        "            'see', 'like', 'know', 'think', 'work', 'time', 'way', 'use', 'using',\n",
        "            'people', 'lot', 'really', 'much', 'would', 'also', 'even', 'still',\n",
        "            'im', 'ive', 'dont', 'cant', 'wont', 'isnt', 'arent', 'wasnt', 'werent'\n",
        "        ])\n",
        "\n",
        "        self.request_count = 0\n",
        "\n",
        "    def log_request(self):\n",
        "        \"\"\"Track API requests\"\"\"\n",
        "        self.request_count += 1\n",
        "        if self.request_count % 10 == 0:\n",
        "            print(f\"   Made {self.request_count} API requests so far...\")\n",
        "\n",
        "    def get_subreddit_posts(self, subreddit_name, time_filter='month', limit=100):\n",
        "        \"\"\"Get posts from a subreddit within a time period\"\"\"\n",
        "        try:\n",
        "            subreddit = self.reddit.subreddit(subreddit_name)\n",
        "            self.log_request()\n",
        "\n",
        "            posts_data = []\n",
        "\n",
        "            # Get top posts from the time period\n",
        "            for post in subreddit.top(time_filter=time_filter, limit=limit):\n",
        "                self.log_request()\n",
        "\n",
        "                # Skip stickied/pinned posts\n",
        "                if post.stickied:\n",
        "                    continue\n",
        "\n",
        "                post_data = {\n",
        "                    'id': post.id,\n",
        "                    'title': post.title,\n",
        "                    'selftext': post.selftext if hasattr(post, 'selftext') else '',\n",
        "                    'score': post.score,\n",
        "                    'num_comments': post.num_comments,\n",
        "                    'created_utc': datetime.fromtimestamp(post.created_utc),\n",
        "                    'url': post.url,\n",
        "                    'subreddit': subreddit_name,\n",
        "                    'author': str(post.author) if post.author else '[deleted]'\n",
        "                }\n",
        "\n",
        "                # Get top comments for additional context\n",
        "                post.comments.replace_more(limit=0)  # Remove \"load more comments\"\n",
        "                comments = []\n",
        "                for comment in post.comments.list()[:10]:  # Top 10 comments\n",
        "                    if hasattr(comment, 'body') and len(comment.body) > 20:\n",
        "                        comments.append(comment.body)\n",
        "\n",
        "                post_data['top_comments'] = comments\n",
        "                posts_data.append(post_data)\n",
        "\n",
        "                # Small delay to be respectful to Reddit's servers\n",
        "                if len(posts_data) % 20 == 0:\n",
        "                    time.sleep(1)\n",
        "\n",
        "            return posts_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching posts from r/{subreddit_name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def filter_posts_by_date(self, posts, start_date, end_date):\n",
        "        \"\"\"Filter posts by date range\"\"\"\n",
        "        filtered = []\n",
        "        for post in posts:\n",
        "            if start_date <= post['created_utc'] <= end_date:\n",
        "                filtered.append(post)\n",
        "        return filtered\n",
        "\n",
        "    def extract_text_content(self, posts):\n",
        "        \"\"\"Extract all text content from posts and comments\"\"\"\n",
        "        all_text = []\n",
        "\n",
        "        for post in posts:\n",
        "            # Combine title and selftext\n",
        "            text_parts = [post['title']]\n",
        "            if post['selftext'] and len(post['selftext']) > 10:\n",
        "                text_parts.append(post['selftext'])\n",
        "\n",
        "            # Add top comments\n",
        "            text_parts.extend(post['top_comments'])\n",
        "\n",
        "            # Join all text for this post\n",
        "            full_text = ' '.join(text_parts)\n",
        "            all_text.append(full_text)\n",
        "\n",
        "        return all_text\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        # Remove Reddit-specific formatting\n",
        "        text = re.sub(r'/r/\\w+|/u/\\w+|u/\\w+|r/\\w+', '', text)\n",
        "        # Remove special characters but keep spaces\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_keywords(self, texts, min_freq=3, max_features=200):\n",
        "        \"\"\"Extract keywords using TF-IDF\"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        # Clean all texts\n",
        "        cleaned_texts = [self.clean_text(text) for text in texts if text.strip()]\n",
        "\n",
        "        if not cleaned_texts:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Use TF-IDF to extract important terms\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=max_features,\n",
        "                ngram_range=(1, 3),  # Include up to 3-word phrases\n",
        "                min_df=min_freq,\n",
        "                max_df=0.8,  # Ignore terms that appear in >80% of documents\n",
        "                stop_words=list(self.stop_words),\n",
        "                token_pattern=r'\\b[a-zA-Z]{3,}\\b'  # Only words with 3+ letters\n",
        "            )\n",
        "\n",
        "            tfidf_matrix = vectorizer.fit_transform(cleaned_texts)\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            scores = tfidf_matrix.sum(axis=0).A1\n",
        "\n",
        "            # Create keyword-score pairs\n",
        "            keyword_scores = list(zip(feature_names, scores))\n",
        "            keyword_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return keyword_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in keyword extraction: {e}\")\n",
        "            return []\n",
        "\n",
        "    def analyze_trends(self, months_back=6, posts_per_subreddit=100):\n",
        "        \"\"\"Analyze keyword trends across AI subreddits\"\"\"\n",
        "        print(f\"Analyzing AI keyword trends across {len(self.ai_subreddits)} subreddits\")\n",
        "        print(f\"Time period: {months_back} months back\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Define time periods\n",
        "        end_date = datetime.now()\n",
        "        mid_date = end_date - timedelta(days=months_back*30//2)  # 3 months ago\n",
        "        start_date = end_date - timedelta(days=months_back*30)   # 6 months ago\n",
        "\n",
        "        print(f\"Period 1 (Earlier): {start_date.date()} to {mid_date.date()}\")\n",
        "        print(f\"Period 2 (Recent):  {mid_date.date()} to {end_date.date()}\")\n",
        "        print()\n",
        "\n",
        "        all_period1_posts = []\n",
        "        all_period2_posts = []\n",
        "        subreddit_stats = {}\n",
        "\n",
        "        for i, subreddit_name in enumerate(self.ai_subreddits):\n",
        "            print(f\"Processing r/{subreddit_name} ({i+1}/{len(self.ai_subreddits)})...\")\n",
        "\n",
        "            # Get posts from the full 6-month period\n",
        "            try:\n",
        "                # For period 1, we need posts from 6 months ago\n",
        "                posts_6m = self.get_subreddit_posts(subreddit_name, 'all', limit=posts_per_subreddit*2)\n",
        "\n",
        "                # Filter posts by date periods\n",
        "                period1_posts = self.filter_posts_by_date(posts_6m, start_date, mid_date)\n",
        "                period2_posts = self.filter_posts_by_date(posts_6m, mid_date, end_date)\n",
        "\n",
        "                # If we don't have enough older posts, get more recent ones\n",
        "                if len(period1_posts) < 20:\n",
        "                    recent_posts = self.get_subreddit_posts(subreddit_name, 'month', limit=posts_per_subreddit)\n",
        "                    period2_posts.extend(recent_posts[:posts_per_subreddit//2])\n",
        "\n",
        "                all_period1_posts.extend(period1_posts)\n",
        "                all_period2_posts.extend(period2_posts)\n",
        "\n",
        "                subreddit_stats[subreddit_name] = {\n",
        "                    'period1_posts': len(period1_posts),\n",
        "                    'period2_posts': len(period2_posts),\n",
        "                    'total_posts': len(period1_posts) + len(period2_posts)\n",
        "                }\n",
        "\n",
        "                print(f\"   Found {len(period1_posts)} posts (period 1), {len(period2_posts)} posts (period 2)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error processing r/{subreddit_name}: {e}\")\n",
        "                subreddit_stats[subreddit_name] = {'error': str(e)}\n",
        "\n",
        "            # Small delay between subreddits\n",
        "            time.sleep(1)\n",
        "\n",
        "        print(f\"\\nTotal posts collected:\")\n",
        "        print(f\"Period 1: {len(all_period1_posts)} posts\")\n",
        "        print(f\"Period 2: {len(all_period2_posts)} posts\")\n",
        "        print(f\"Total API requests made: {self.request_count}\")\n",
        "\n",
        "        # Extract text content\n",
        "        print(\"\\nExtracting text content...\")\n",
        "        period1_texts = self.extract_text_content(all_period1_posts)\n",
        "        period2_texts = self.extract_text_content(all_period2_posts)\n",
        "\n",
        "        # Extract keywords\n",
        "        print(\"Extracting keywords for period 1...\")\n",
        "        keywords1 = self.extract_keywords(period1_texts, min_freq=2)\n",
        "\n",
        "        print(\"Extracting keywords for period 2...\")\n",
        "        keywords2 = self.extract_keywords(period2_texts, min_freq=2)\n",
        "\n",
        "        return {\n",
        "            'period1_keywords': dict(keywords1),\n",
        "            'period2_keywords': dict(keywords2),\n",
        "            'period1_posts': all_period1_posts,\n",
        "            'period2_posts': all_period2_posts,\n",
        "            'period1_texts': period1_texts,\n",
        "            'period2_texts': period2_texts,\n",
        "            'subreddit_stats': subreddit_stats,\n",
        "            'date_ranges': {\n",
        "                'period1': f\"{start_date.date()} to {mid_date.date()}\",\n",
        "                'period2': f\"{mid_date.date()} to {end_date.date()}\"\n",
        "            },\n",
        "            'api_requests': self.request_count\n",
        "        }\n",
        "\n",
        "    def create_visualizations(self, trend_data):\n",
        "        \"\"\"Create comprehensive visualizations of keyword trends\"\"\"\n",
        "        period1_keywords = trend_data['period1_keywords']\n",
        "        period2_keywords = trend_data['period2_keywords']\n",
        "\n",
        "        # Create comparison dataset\n",
        "        all_keywords = set(period1_keywords.keys()) | set(period2_keywords.keys())\n",
        "\n",
        "        comparison_data = []\n",
        "        for keyword in all_keywords:\n",
        "            score1 = period1_keywords.get(keyword, 0)\n",
        "            score2 = period2_keywords.get(keyword, 0)\n",
        "            difference = score2 - score1\n",
        "\n",
        "            # Calculate percentage change (avoid division by zero)\n",
        "            if score1 > 0:\n",
        "                percent_change = ((score2 - score1) / score1) * 100\n",
        "            elif score2 > 0:\n",
        "                percent_change = 200  # New keyword\n",
        "            else:\n",
        "                percent_change = 0\n",
        "\n",
        "            comparison_data.append({\n",
        "                'keyword': keyword,\n",
        "                'period1_score': score1,\n",
        "                'period2_score': score2,\n",
        "                'difference': difference,\n",
        "                'percent_change': percent_change,\n",
        "                'avg_score': (score1 + score2) / 2\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(comparison_data)\n",
        "        df = df.sort_values('difference', key=abs, ascending=False)\n",
        "\n",
        "        # Create visualizations\n",
        "        fig = plt.figure(figsize=(20, 24))\n",
        "\n",
        "        # 1. Top growing keywords\n",
        "        plt.subplot(3, 2, 1)\n",
        "        top_growing = df.nlargest(20, 'difference')\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_growing['difference']]\n",
        "        plt.barh(range(len(top_growing)), top_growing['difference'], color=colors, alpha=0.7)\n",
        "        plt.yticks(range(len(top_growing)), top_growing['keyword'])\n",
        "        plt.title('Top 20 Growing Keywords (Recent vs Earlier Period)', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('TF-IDF Score Difference')\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        # 2. Top declining keywords\n",
        "        plt.subplot(3, 2, 2)\n",
        "        top_declining = df.nsmallest(20, 'difference')\n",
        "        colors = ['red' if x < 0 else 'green' for x in top_declining['difference']]\n",
        "        plt.barh(range(len(top_declining)), top_declining['difference'], color=colors, alpha=0.7)\n",
        "        plt.yticks(range(len(top_declining)), top_declining['keyword'])\n",
        "        plt.title('Top 20 Declining Keywords (Recent vs Earlier Period)', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('TF-IDF Score Difference')\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        # 3. Scatter plot: Period 1 vs Period 2 scores\n",
        "        plt.subplot(3, 2, 3)\n",
        "        plt.scatter(df['period1_score'], df['period2_score'], alpha=0.6, s=60)\n",
        "        max_score = max(df[['period1_score', 'period2_score']].max())\n",
        "        plt.plot([0, max_score], [0, max_score], 'r--', alpha=0.5, label='Equal usage line')\n",
        "        plt.xlabel('Earlier Period TF-IDF Score')\n",
        "        plt.ylabel('Recent Period TF-IDF Score')\n",
        "        plt.title('Keyword Usage Comparison: Earlier vs Recent Period')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        # Annotate notable keywords\n",
        "        for _, row in df.head(15).iterrows():\n",
        "            if abs(row['difference']) > df['difference'].std() * 1.5:\n",
        "                plt.annotate(row['keyword'][:15],\n",
        "                           (row['period1_score'], row['period2_score']),\n",
        "                           xytext=(5, 5), textcoords='offset points',\n",
        "                           fontsize=8, alpha=0.8,\n",
        "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
        "\n",
        "        # 4. Percentage change distribution\n",
        "        plt.subplot(3, 2, 4)\n",
        "        # Filter extreme outliers for better visualization\n",
        "        filtered_changes = df[abs(df['percent_change']) < 500]['percent_change']\n",
        "        plt.hist(filtered_changes, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='No change')\n",
        "        plt.xlabel('Percentage Change (%)')\n",
        "        plt.ylabel('Number of Keywords')\n",
        "        plt.title('Distribution of Keyword Usage Changes')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        # 5. Word cloud for Period 1\n",
        "        plt.subplot(3, 2, 5)\n",
        "        if trend_data['period1_texts']:\n",
        "            text1 = ' '.join([self.clean_text(text) for text in trend_data['period1_texts']])\n",
        "            # Remove common words for better visualization\n",
        "            for word in ['ai', 'model', 'data', 'learning', 'machine', 'use', 'work', 'new', 'get']:\n",
        "                text1 = re.sub(rf'\\b{word}\\b', '', text1)\n",
        "\n",
        "            try:\n",
        "                wordcloud1 = WordCloud(\n",
        "                    width=800, height=400,\n",
        "                    background_color='white',\n",
        "                    stopwords=self.stop_words,\n",
        "                    max_words=100,\n",
        "                    colormap='viridis'\n",
        "                ).generate(text1)\n",
        "                plt.imshow(wordcloud1, interpolation='bilinear')\n",
        "                plt.title(f'Word Cloud - {trend_data[\"date_ranges\"][\"period1\"]}', fontsize=12)\n",
        "                plt.axis('off')\n",
        "            except:\n",
        "                plt.text(0.5, 0.5, 'Word Cloud\\nGeneration Failed', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "                plt.title(f'Period 1: {trend_data[\"date_ranges\"][\"period1\"]}')\n",
        "\n",
        "        # 6. Word cloud for Period 2\n",
        "        plt.subplot(3, 2, 6)\n",
        "        if trend_data['period2_texts']:\n",
        "            text2 = ' '.join([self.clean_text(text) for text in trend_data['period2_texts']])\n",
        "            # Remove common words for better visualization\n",
        "            for word in ['ai', 'model', 'data', 'learning', 'machine', 'use', 'work', 'new', 'get']:\n",
        "                text2 = re.sub(rf'\\b{word}\\b', '', text2)\n",
        "\n",
        "            try:\n",
        "                wordcloud2 = WordCloud(\n",
        "                    width=800, height=400,\n",
        "                    background_color='white',\n",
        "                    stopwords=self.stop_words,\n",
        "                    max_words=100,\n",
        "                    colormap='plasma'\n",
        "                ).generate(text2)\n",
        "                plt.imshow(wordcloud2, interpolation='bilinear')\n",
        "                plt.title(f'Word Cloud - {trend_data[\"date_ranges\"][\"period2\"]}', fontsize=12)\n",
        "                plt.axis('off')\n",
        "            except:\n",
        "                plt.text(0.5, 0.5, 'Word Cloud\\nGeneration Failed', ha='center', va='center', transform=plt.gca().transAxes)\n",
        "                plt.title(f'Period 2: {trend_data[\"date_ranges\"][\"period2\"]}')\n",
        "\n",
        "        plt.suptitle('Reddit AI Keyword Trends Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print detailed results\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ” REDDIT AI KEYWORD TRENDS ANALYSIS RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nðŸ“Š DATA COLLECTION SUMMARY:\")\n",
        "        print(f\"   â€¢ Total posts analyzed: {len(trend_data['period1_posts']) + len(trend_data['period2_posts'])}\")\n",
        "        print(f\"   â€¢ Period 1 posts: {len(trend_data['period1_posts'])}\")\n",
        "        print(f\"   â€¢ Period 2 posts: {len(trend_data['period2_posts'])}\")\n",
        "        print(f\"   â€¢ API requests made: {trend_data['api_requests']}\")\n",
        "        print(f\"   â€¢ Subreddits analyzed: {len([s for s in trend_data['subreddit_stats'] if 'error' not in trend_data['subreddit_stats'][s]])}\")\n",
        "\n",
        "        print(f\"\\nðŸš€ TOP 15 GROWING KEYWORDS (Gaining Popularity):\")\n",
        "        for i, (_, row) in enumerate(df.nlargest(15, 'difference').iterrows(), 1):\n",
        "            print(f\"   {i:2d}. {row['keyword']:30} | +{row['difference']:.3f} ({row['percent_change']:+6.1f}%)\")\n",
        "\n",
        "        print(f\"\\nðŸ“‰ TOP 15 DECLINING KEYWORDS (Losing Popularity):\")\n",
        "        for i, (_, row) in enumerate(df.nsmallest(15, 'difference').iterrows(), 1):\n",
        "            print(f\"   {i:2d}. {row['keyword']:30} | {row['difference']:.3f} ({row['percent_change']:+6.1f}%)\")\n",
        "\n",
        "        print(f\"\\nðŸ”¥ MOST DISCUSSED KEYWORDS (Highest Recent Activity):\")\n",
        "        for i, (_, row) in enumerate(df.nlargest(15, 'period2_score').iterrows(), 1):\n",
        "            print(f\"   {i:2d}. {row['keyword']:30} | Score: {row['period2_score']:.3f}\")\n",
        "\n",
        "        print(f\"\\nðŸ“ˆ SUBREDDIT BREAKDOWN:\")\n",
        "        for subreddit, stats in trend_data['subreddit_stats'].items():\n",
        "            if 'error' not in stats:\n",
        "                total = stats['period1_posts'] + stats['period2_posts']\n",
        "                print(f\"   r/{subreddit:20} | {total:3d} posts ({stats['period1_posts']} + {stats['period2_posts']})\")\n",
        "            else:\n",
        "                print(f\"   r/{subreddit:20} | Error: {stats['error'][:50]}...\")\n",
        "\n",
        "        return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Reddit AI Keyword Trend Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"This tool analyzes AI discussion trends across major AI subreddits\")\n",
        "    print(\"comparing keyword usage from 6 months ago vs recent 3 months.\\n\")\n",
        "\n",
        "    # Check if API credentials are set\n",
        "    if REDDIT_CLIENT_ID == \"your_client_id_here\":\n",
        "        print(\"âš ï¸  SETUP REQUIRED:\")\n",
        "        print(\"Please set up Reddit API credentials:\")\n",
        "        print(\"1. Go to https://www.reddit.com/prefs/apps\")\n",
        "        print(\"2. Click 'Create App' or 'Create Another App'\")\n",
        "        print(\"3. Choose 'script' type\")\n",
        "        print(\"4. Copy the client ID and secret to this notebook\")\n",
        "        print(\"5. Update REDDIT_USER_AGENT with your username\")\n",
        "        print(\"\\nReddit API is free and much more generous than Twitter!\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Initialize analyzer\n",
        "        print(\"ðŸ”— Connecting to Reddit API...\")\n",
        "        analyzer = RedditAIAnalyzer(\n",
        "            REDDIT_CLIENT_ID,\n",
        "            REDDIT_CLIENT_SECRET,\n",
        "            REDDIT_USER_AGENT\n",
        "        )\n",
        "\n",
        "        # Test connection\n",
        "        test_subreddit = analyzer.reddit.subreddit('MachineLearning')\n",
        "        print(f\"âœ… Connected! Testing with r/MachineLearning ({test_subreddit.subscribers:,} subscribers)\")\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Target subreddits ({len(analyzer.ai_subreddits)}):\")\n",
        "        for sub in analyzer.ai_subreddits:\n",
        "            print(f\"   â€¢ r/{sub}\")\n",
        "\n",
        "        # Run analysis\n",
        "        print(f\"\\nðŸš€ Starting analysis...\")\n",
        "        trend_data = analyzer.analyze_trends(months_back=6, posts_per_subreddit=50)\n",
        "\n",
        "        # Create visualizations\n",
        "        print(f\"\\nðŸ“Š Creating visualizations...\")\n",
        "        comparison_df = analyzer.create_visualizations(trend_data)\n",
        "\n",
        "        print(f\"\\nâœ… Analysis complete!\")\n",
        "        print(f\"   â€¢ Processed {len(trend_data['period1_posts']) + len(trend_data['period2_posts'])} total posts\")\n",
        "        print(f\"   â€¢ Used {trend_data['api_requests']} API requests\")\n",
        "        print(f\"   â€¢ Analyzed {len(comparison_df)} unique keywords\")\n",
        "\n",
        "        return comparison_df, trend_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during analysis: {e}\")\n",
        "        print(f\"   This might be due to API credentials or network connectivity.\")\n",
        "        return None, None\n",
        "\n",
        "# Execute the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    result_df, analysis_data = main()\n",
        "\n",
        "# Optional: Save results to CSV\n",
        "if result_df is not None:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"reddit_ai_trends_{timestamp}.csv\"\n",
        "    result_df.to_csv(filename, index=False)\n",
        "    print(f\"\\nðŸ’¾ Results saved to {filename}\")"
      ]
    }
  ]
}